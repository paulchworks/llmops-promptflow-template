
parameters:
 - name: exec_environment
   displayName: "Execution Environment"
   default: "dev"
 - name: use_case_base_path
   displayName: "Base path of model to execute"
 - name: RESOURCE_GROUP_NAME
   displayName: "Resource group name"
 - name: WORKSPACE_NAME
   displayName: "Workspace name"
 - name: env_vars
   displayName: "env vars"

stages:
    - stage: build_validation
      displayName: build_validation
      jobs:
        - template: build_validation_pipeline.yml
          parameters:
            use_case_base_path: ${{ parameters.use_case_base_path }}

    - stage: execute_training_job
      displayName: execute_training_job
      dependsOn: 
      - build_validation
      variables: 
      - name: WORKSPACE_NAME
        value: ${{ parameters.WORKSPACE_NAME }}
      - name: RESOURCE_GROUP_NAME
        value: ${{ parameters.RESOURCE_GROUP_NAME }}
      jobs:
      - job: Execute_ml_Job_Pipeline
        steps:
        - template: templates/get_connection_details.yml

        - template: templates/configure_azureml_agent.yml
          parameters:
            base_path: ${{ parameters.use_case_base_path }}

        - task: Bash@3
          displayName: Create .env file
          inputs:
            targetType: 'inline'
            script: |
              echo "$(env_vars)" | tr ' ' '\n' > .env
          env:
            env_vars: $(env_vars)

        - task: Bash@3
          displayName: Load .env file
          inputs:
            targetType: 'inline'
            script: |
              python -c "from dotenv import load_dotenv; load_dotenv()"

        #=====================================
        # Registers experiment dataset in Azure ML as Data Asset
        # Reads appropriate field values from experiment.yaml or experiment.<env>.yaml
        #=====================================           
        - template: templates/execute_python_code.yml
          parameters:
            step_name: "Register Training Data"
            script_parameter: |
              python -m llmops.common.register_data_asset \
                --subscription_id "$(SUBSCRIPTION_ID)" \
                --base_path ${{ parameters.use_case_base_path }} \
                --env_name ${{ parameters.exec_environment }}

        #=====================================
        # Executes Standard flow for a scenario
        # Generates Reports for each RUN as well as consolidated one
        # Execute a RUN for each unique variant combination (keeping default variant id for other nodes)
        # Loads appropriate experiment data from Azure ML data asset
        # Reads appropriate field values from experiment.yaml or experiment.<env>.yaml
        # Prompt Flow connections should pre-exist 
        # used automatic (serverless) runtime by default
        #=====================================
        - template: templates/execute_python_code.yml
          parameters:
            step_name: "Execute Standard Flow"
            script_parameter: |
              python -m llmops.common.prompt_pipeline \
                --subscription_id "$(SUBSCRIPTION_ID)" \
                --build_id $(BUILD.BUILDID) \
                --env_name ${{ parameters.exec_environment }} \
                --base_path ${{ parameters.use_case_base_path }} \
                --output_file run_id.txt
